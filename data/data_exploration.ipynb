{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# NLI Data Preprocessing Exploration\n",
    "\n",
    "Walk through the data preprocessing pipeline step by step:\n",
    "1. Raw CSV triplets (anchor, positive, hard negative)\n",
    "2. Tokenization with prefix-renaming\n",
    "3. Final tensor format consumed by `ContrastiveTrainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raw-header",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Raw CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "load-raw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 275,601\n",
      "Columns: ['sent0', 'sent1', 'hard_neg']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "      <th>hard_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>You lose the things to the following level if ...</td>\n",
       "      <td>They never perform recalls on anything.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One of our number will carry out your instruct...</td>\n",
       "      <td>A member of my team will execute your orders w...</td>\n",
       "      <td>We have no one free at the moment so you have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do you know? All this is their information...</td>\n",
       "      <td>This information belongs to them.</td>\n",
       "      <td>They have no information at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sent0  \\\n",
       "0  you know during the season and i guess at at y...   \n",
       "1  One of our number will carry out your instruct...   \n",
       "2  How do you know? All this is their information...   \n",
       "\n",
       "                                               sent1  \\\n",
       "0  You lose the things to the following level if ...   \n",
       "1  A member of my team will execute your orders w...   \n",
       "2                  This information belongs to them.   \n",
       "\n",
       "                                            hard_neg  \n",
       "0            They never perform recalls on anything.  \n",
       "1  We have no one free at the moment so you have ...  \n",
       "2                   They have no information at all.  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Quick peek with pandas\n",
    "df = pd.read_csv('nli_for_simcse.csv')\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "show-triplet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANCHOR (sent0):\n",
      "  One of our number will carry out your instructions minutely.\n",
      "\n",
      "POSITIVE (sent1) — entailment/paraphrase:\n",
      "  A member of my team will execute your orders with immense precision.\n",
      "\n",
      "HARD NEGATIVE (hard_neg) — contradiction:\n",
      "  We have no one free at the moment so you have to take action yourself.\n"
     ]
    }
   ],
   "source": [
    "# Look at a single triplet in full\n",
    "row = df.iloc[1]\n",
    "print(\"ANCHOR (sent0):\")\n",
    "print(f\"  {row['sent0']}\")\n",
    "print()\n",
    "print(\"POSITIVE (sent1) — entailment/paraphrase:\")\n",
    "print(f\"  {row['sent1']}\")\n",
    "print()\n",
    "print(\"HARD NEGATIVE (hard_neg) — contradiction:\")\n",
    "print(f\"  {row['hard_neg']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "length-stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent0       mean=15.7  median=13  max=382  min=1\n",
      "sent1       mean=8.1  median=7  max=61  min=1\n",
      "hard_neg    mean=8.2  median=8  max=56  min=1\n"
     ]
    }
   ],
   "source": [
    "# Sentence length distributions (in words)\n",
    "for col in ['sent0', 'sent1', 'hard_neg']:\n",
    "    lengths = df[col].str.split().str.len()\n",
    "    print(f\"{col:10s}  mean={lengths.mean():.1f}  median={lengths.median():.0f}  \"\n",
    "          f\"max={lengths.max()}  min={lengths.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenize-header",
   "metadata": {},
   "source": [
    "## 2. Tokenization Step-by-Step\n",
    "\n",
    "The preprocessor tokenizes each column independently, then renames keys with a prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-tokenizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 122,753\n",
      "EOS token: '</s>' (id=2)\n",
      "Pad token: '</s>' (id=2)\n",
      "add_eos_token: True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../pretrained/MiniCPM-2B-dpo-bf16/', local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token!r} (id={tokenizer.eos_token_id})\")\n",
    "print(f\"Pad token: {tokenizer.pad_token!r} (id={tokenizer.pad_token_id})\")\n",
    "print(f\"add_eos_token: {tokenizer.add_eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tokenize-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'One of our number will carry out your instructions minutely.'\n",
      "\n",
      "Token count (no padding): 14\n",
      "Token IDs: [1, 4502, 1379, 1807, 2189, 1601, 7879, 1707, 1615, 12902, 2204, 11530, 72, 2]\n",
      "\n",
      "Decoded tokens: ['<s>', 'One', 'of', 'our', 'number', 'will', 'carry', 'out', 'your', 'instructions', 'min', 'utely', '.', '</s>']\n",
      "Last token: '</s>' (should be EOS)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a single sentence to see what happens\n",
    "sample_text = row['sent0']\n",
    "print(f\"Text: {sample_text!r}\")\n",
    "print()\n",
    "\n",
    "# Without padding (to see real token count)\n",
    "tokens_raw = tokenizer(sample_text)\n",
    "print(f\"Token count (no padding): {len(tokens_raw['input_ids'])}\")\n",
    "print(f\"Token IDs: {tokens_raw['input_ids']}\")\n",
    "print()\n",
    "\n",
    "# Decode back to see individual tokens\n",
    "token_strings = [tokenizer.decode(tid) for tid in tokens_raw['input_ids']]\n",
    "print(f\"Decoded tokens: {token_strings}\")\n",
    "print(f\"Last token: {token_strings[-1]!r} (should be EOS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tokenize-padded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([150])\n",
      "Real tokens: 14, Padding tokens: 136\n",
      "\n",
      "First 20 IDs:    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Attention mask:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Last 10 IDs:     [2189, 1601, 7879, 1707, 1615, 12902, 2204, 11530, 72, 2]\n",
      "Attention mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Pad token ID = 2 (same as EOS = 2)\n"
     ]
    }
   ],
   "source": [
    "# With max_length=150 padding (what the preprocessor does)\n",
    "tokens_padded = tokenizer(sample_text, padding='max_length', truncation=True,\n",
    "                          return_tensors='pt', max_length=150)\n",
    "\n",
    "input_ids = tokens_padded['input_ids'][0]\n",
    "attention_mask = tokens_padded['attention_mask'][0]\n",
    "\n",
    "real_tokens = attention_mask.sum().item()\n",
    "pad_tokens = (attention_mask == 0).sum().item()\n",
    "\n",
    "print(f\"Shape: {input_ids.shape}\")\n",
    "print(f\"Real tokens: {real_tokens}, Padding tokens: {pad_tokens}\")\n",
    "print()\n",
    "print(f\"First 20 IDs:    {input_ids[:20].tolist()}\")\n",
    "print(f\"Attention mask:  {attention_mask[:20].tolist()}\")\n",
    "print()\n",
    "print(f\"Last 10 IDs:     {input_ids[-10:].tolist()}\")\n",
    "print(f\"Attention mask:  {attention_mask[-10:].tolist()}\")\n",
    "print(f\"\\nPad token ID = {tokenizer.pad_token_id} (same as EOS = {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize-rename",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The _tokenize method renames keys with a prefix\n",
    "# This is how 3 sentences coexist in the same dataset row\n",
    "\n",
    "def tokenize_with_prefix(text, prefix):\n",
    "    \"\"\"Mirrors NLIPreprocess._tokenize()\"\"\"\n",
    "    out = tokenizer(text, padding='max_length', truncation=True,\n",
    "                    return_tensors='pt', max_length=150)\n",
    "    out[f'{prefix}_input_ids'] = out.pop('input_ids')\n",
    "    out[f'{prefix}_attention_mask'] = out.pop('attention_mask')\n",
    "    return out\n",
    "\n",
    "result = tokenize_with_prefix(sample_text, 'sent0')\n",
    "print(f\"Keys after renaming: {list(result.keys())}\")\n",
    "print(f\"sent0_input_ids shape: {result['sent0_input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token-dist-header",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Token Length Distribution\n",
    "\n",
    "Check how many sentences get truncated at `max_length=150`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "token-dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Sample 5000 rows for speed\n",
    "sample_df = df.sample(n=min(5000, len(df)), random_state=42)\n",
    "\n",
    "token_lengths = {col: [] for col in ['sent0', 'sent1', 'hard_neg']}\n",
    "\n",
    "for _, r in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Tokenizing sample\", unit=\"row\"):\n",
    "    for col in token_lengths:\n",
    "        toks = tokenizer(r[col], add_special_tokens=True)\n",
    "        token_lengths[col].append(len(toks['input_ids']))\n",
    "\n",
    "for col, lengths in token_lengths.items():\n",
    "    lengths_arr = pd.Series(lengths)\n",
    "    truncated = (lengths_arr >= 150).sum()\n",
    "    print(f\"{col:10s}  mean={lengths_arr.mean():.1f}  p95={lengths_arr.quantile(0.95):.0f}  \"\n",
    "          f\"max={lengths_arr.max()}  truncated@150={truncated} ({100*truncated/len(lengths_arr):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-header",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Processed Dataset (what the trainer sees)\n",
    "\n",
    "Load the preprocessed Arrow dataset from `data/processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-processed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk('./processed/')\n",
    "print(f\"Dataset: {ds}\")\n",
    "print(f\"Columns: {ds.column_names}\")\n",
    "print(f\"Features: {ds.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a single processed example\n",
    "sample = ds[0]\n",
    "for key in sorted(sample.keys()):\n",
    "    val = sample[key]\n",
    "    if hasattr(val, 'shape'):\n",
    "        print(f\"{key:30s}  shape={val.shape}  dtype={val.dtype}\")\n",
    "    else:\n",
    "        print(f\"{key:30s}  type={type(val).__name__}  len={len(val) if hasattr(val, '__len__') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decode-processed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the processed tokens back to text to verify correctness\n",
    "sample = ds[1]\n",
    "\n",
    "for prefix in ['sent0', 'sent1', 'hard_neg']:\n",
    "    ids = sample[f'{prefix}_input_ids']\n",
    "    mask = sample[f'{prefix}_attention_mask']\n",
    "    real_len = mask.sum().item() if hasattr(mask, 'sum') else sum(mask)\n",
    "    \n",
    "    # Decode only real tokens (not padding)\n",
    "    real_ids = ids[:real_len] if hasattr(ids, '__getitem__') else ids\n",
    "    decoded = tokenizer.decode(real_ids, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"{prefix} ({real_len} tokens):\")\n",
    "    print(f\"  {decoded}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainer-header",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. How the Trainer Consumes This\n",
    "\n",
    "The `ContrastiveTrainer.compute_loss()` unpacks each batch into 3 dicts:\n",
    "\n",
    "```python\n",
    "sent0 = {'input_ids': inputs['sent0_input_ids'],\n",
    "         'attention_mask': inputs['sent0_attention_mask']}\n",
    "# ... same for sent1, hard_neg\n",
    "\n",
    "sent0_embed = model(**sent0, output_hidden_states=True).hidden_states[-1][:, -1, :]\n",
    "```\n",
    "\n",
    "Each gets fed through the model separately. The **last hidden state at the last token position** (EOS) is used as the embedding. Then InfoNCE loss is computed over (anchor, positive, hard_negative) triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simulate-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Simulate what a DataLoader batch looks like\n",
    "batch_size = 4\n",
    "batch = ds[:batch_size]\n",
    "\n",
    "# The trainer unpacks it like this:\n",
    "sent0 = {'input_ids': torch.tensor(batch['sent0_input_ids']),\n",
    "         'attention_mask': torch.tensor(batch['sent0_attention_mask'])}\n",
    "sent1 = {'input_ids': torch.tensor(batch['sent1_input_ids']),\n",
    "         'attention_mask': torch.tensor(batch['sent1_attention_mask'])}\n",
    "hard_neg = {'input_ids': torch.tensor(batch['hard_neg_input_ids']),\n",
    "            'attention_mask': torch.tensor(batch['hard_neg_attention_mask'])}\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "for name, d in [('sent0', sent0), ('sent1', sent1), ('hard_neg', hard_neg)]:\n",
    "    print(f\"{name:10s}  input_ids={d['input_ids'].shape}  attention_mask={d['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eos-position",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is the EOS token (= the embedding extraction point) in each sequence?\n",
    "# Since we pad to max_length and add_eos_token=True, EOS is the last real token.\n",
    "\n",
    "for name, d in [('sent0', sent0), ('sent1', sent1), ('hard_neg', hard_neg)]:\n",
    "    # Last real token position = sum of attention_mask - 1\n",
    "    real_lengths = d['attention_mask'].sum(dim=1)\n",
    "    last_positions = real_lengths - 1\n",
    "    \n",
    "    # Check that the token at that position is EOS\n",
    "    for i in range(batch_size):\n",
    "        pos = last_positions[i].item()\n",
    "        token_id = d['input_ids'][i, pos].item()\n",
    "        is_eos = token_id == tokenizer.eos_token_id\n",
    "        print(f\"{name}[{i}]: real_len={real_lengths[i].item()}, \"\n",
    "              f\"last_token_pos={pos}, token_id={token_id}, is_EOS={is_eos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "note-header",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Each row has 3 sentences: **anchor** (sent0), **positive** (sent1), **hard negative** (hard_neg)\n",
    "- All tokenized to fixed length 150 with padding, producing 6 columns: `{prefix}_input_ids` + `{prefix}_attention_mask`\n",
    "- `set_format(\"torch\")` makes the dataset return PyTorch tensors — no custom collator needed\n",
    "- The model extracts embeddings from the **last token (EOS)** of the last hidden layer\n",
    "- `pad_token = eos_token` means padding tokens have the same ID as EOS, but the **attention mask** distinguishes them — the model only attends to real tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
