---
title: "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"
source: "https://arxiv.org/html/2404.05961v2"
author:
published:
created: 2026-02-20
description:
tags:
  - "clippings"
---
Authors: achieve the best HTML results from your LaTeX submissions by following these [best practices](https://info.arxiv.org/help/submit_latex_best_practices.html).

arXiv:2404.05961v2 \[cs.CL\] 21 Aug 2024

Parishad BehnamGhader <sup><span>∗,⋄</span></sup> Vaibhav Adlakha <sup><span>∗,⋄,†</span></sup> Marius Mosbach <sup><span>⋄</span></sup>  
Dzmitry Bahdanau <sup><span>†</span></sup> Nicolas Chapados <sup><span>†</span></sup> Siva Reddy <sup><span>⋄,†,‡</span></sup>  
<sup><span>⋄</span></sup> McGill University, Mila <sup><span>†</span></sup> ServiceNow Research <sup><span>‡</span></sup> Facebook CIFAR AI Chair  
{parishad.behnamghader,vaibhav.adlakha,marius.mosbach}@mila.quebec  
<sup><span>∗</span></sup> Equal contribution.

###### Abstract

Large decoder-only language models (LLMs) are the state-of-the-art models on most of today’s NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 4 popular LLMs ranging from 1.3B to 8B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data (as of May 24, 2024). Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.

## 1 Introduction

Text embedding models aim to encode the semantic content of natural language text in vector representations which then facilitate various natural language processing (NLP) tasks, such as semantic textual similarity, information retrieval, and clustering. For many years, the dominating paradigm for building such models relied on pre-trained bidirectional encoders or encoder-decoders such as BERT [^13] and T5 [^41], which are typically adapted for text embedding tasks by following a multi-step training pipeline consisting of weakly- and fully-supervised contrastive training [^38]. Only recently, the community started to adopt decoder-only LLMs for embedding text [^34].

We speculate that the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations. At any given layer, causal attention limits token interactions, ensuring that the representation of a token at position $i$ is influenced solely by the representations of preceding tokens at positions $0,1,\dots,i-1$ . Although this limitation is necessary for generative capabilities, it is sub-optimal for text embeddings as it prevents the representations from capturing information across the entire input sequence.

![Refer to caption](https://arxiv.org/html/2404.05961v2/x1.png)

Figure 1: The 3 steps of LLM2Vec. First, we enable bidirectional attention to overcome the restrictions of causal attention (Bi). Second, we adapt the model to use bidirectional attention by masked next token prediction training (MNTP). Third, we apply unsupervised contrastive learning with mean pooling to learn better sequence representations (SimCSE).

Overcoming this architectural limitation of decoder-only LLMs for text embedding tasks is highly appealing as these models come with several advantages compared to their encoder-only counterparts.<sup>1</sup> <sup>1</sup> 1 We acknowledge that there are also several challenges associated with the large size of these models and provide a discussion in [Appendix A](https://arxiv.org/html/2404.05961v2/2404.05961v2#A1 "Appendix A Limitations ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"). During pre-training, decoder-only LLMs learn from all input tokens and not just a small percentage [^13], which—given the same amount of training data—makes them much more sample-efficient than encoder-only models [^9]. Moreover, there exists a rich ecosystem around these models, with extensive tooling and well tested pre-training recipes, which has resulted in continuous improvement of these models by the community. Lastly, recent work on instruction fine-tuning and learning from human preferences has resulted in decoder-only LLMs that excel at instruction following [^52], making them an ideal choice for building universal text embedding models that generalize across a large variety of tasks using instructions.

In this work, we provide a simple unsupervised approach, termed LLM2Vec, which can be used to transform *any* pre-trained decoder-only LLM into a (universal) text encoder. As shown in [Figure 1](https://arxiv.org/html/2404.05961v2/2404.05961v2#S1.F1 "In 1 Introduction ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"), LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. Crucially, LLM2Vec does not require any labeled data and is highly data- and parameter-efficient.

We apply LLM2vec to 4 decoder-only LLMs ranging from 1.3B to 8B parameters (S-LLaMA-1.3B, LLaMA-2-7B, Mistral-7B, Meta-LLaMA-3-8B) and evaluate the resulting models on word- and sequence-level tasks. On word-level tasks (chunking, named-entity recognition, and part-of-speech tagging), LLM2Vec-transformed models outperform strong encoder-only models by a large margin, demonstrating its effectiveness for producing rich contextualized token representations. On the Massive Text Embeddings Benchmark (MTEB), LLM2Vec-transformed models set a new state-of-the-art for unsupervised models, with our best model reaching a score of $56.8$ . Additionally, we combine LLM2Vec with supervised contrastive training and achieve a new state-of-the-art performance among models that train only on publicly available data. Beyond our strong empirical results, we provide an extensive analysis of how LLM2Vec affects the representations of the underlying model and reveal an intriguing property of Mistral-7B, which can handle bidirectional attention without any fine-tuning.

Overall, our work demonstrates that decoder-only LLMs are indeed capable of producing universal text embedding and only very little adaptation is required to reveal this ability. Our code and pre-trained models is publicly available at [https://github.com/McGill-NLP/llm2vec](https://github.com/McGill-NLP/llm2vec) and [McGill-NLP-llm2vec-Code-for-LLM2Vec-Large-Language-Models-Are-Secretly-Powerf](McGill-NLP-llm2vec-Code-for-LLM2Vec-Large-Language-Models-Are-Secretly-Powerf.md)

## 2 LLM2Vec

### 2.1 Three simple ingredients

##### Enabling bidirectional attention

The first step of the LLM2Vec approach is to replace the causal attention mask of decoder-only LLMs by an all-ones matrix (see [Section B.1](https://arxiv.org/html/2404.05961v2/2404.05961v2#A2.SS1 "B.1 Self-attention ‣ Appendix B Background ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") for background on the self-attention). This gives each token access to every other token in the sequence, converting it into a bidirectional LLM [^13]. However, it is not a priori clear, why this should lead to better sequence representations. After all, the decoder-only LLM was not trained to attend to future tokens and therefore, this naive approach might even lead to worse representations. As we show, simply enabling bidirectional attention does indeed decrease in embedding performance for most models. We can however easily adapt a model to make use of its bidirectional attention.

##### Masked next token prediction

We use a simple strategy to make the model aware of its bidirectional attention by adapting it via masked next token prediction (MNTP). MNTP is a training objective that combines next token prediction with masked language modeling [^31]. Given an arbitrary sequence $\mathbf{x}=(x_{1},x_{2},\ldots,x_{N})$ as input, we first mask a fraction of the input tokens and then train the model to predict the masked tokens based on the past and future context. Crucially, when predicting a masked token at position $i$ , we compute the loss based on the logits obtained from the token representation at the previous position $i-1$ , not the masked position itself (see [Figure 1](https://arxiv.org/html/2404.05961v2/2404.05961v2#S1.F1 "In 1 Introduction ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders")).

##### Unsupervised contrastive learning

While the previous two steps of the LLM2Vec recipe can transform any decoder-only LLM into an encoder for word-level tasks, they might not be sufficient for sequence representations. Unlike bidirectional encoders that include a next sentence prediction objective in their pre-training objectives [^13], decoder-only LLMs are not explicitly trained to capture the context of the entire sequence. To fill this gap, we apply unsupervised contrastive learning via SimCSE [^16]. Specifically, given an input sentence, it is passed through the model twice with independently sampled dropout masks, resulting in two different representations for the same sentence. The model is trained to maximize the similarity between these two representations while minimizing the similarity with representations of other sentences in the batch. Crucially, this step does not require any sentence pair data and can be applied using any collection of sentences. We use a pooling operation on the word representations to get the sentence representation (more details in [Section 3.2](https://arxiv.org/html/2404.05961v2/2404.05961v2#S3.SS2 "3.2 Evaluation on sequence-level tasks ‣ 3 LLM2Vec-transformed models are strong unsupervised text embedders ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders")).

### 2.2 Transforming decoder-only LLMs with LLM2Vec

##### Models

For most of our results, we experiment with 3 different decoder-only LLMs ranging from 1.3B to 7B parameters: Sheared-LLaMA-1.3B (S-LLaMA-1.3B, [^54]), Llama-2-7B-chat (LLaMA-2-7B, [^48]), and Mistral-7B-Instruct-v0.2 (Mistral-7B, [^20]). In [Tables 1](https://arxiv.org/html/2404.05961v2/2404.05961v2#S3.T1 "In Results on our 15 task subset of MTEB ‣ 3.2 Evaluation on sequence-level tasks ‣ 3 LLM2Vec-transformed models are strong unsupervised text embedders ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") and  [2](https://arxiv.org/html/2404.05961v2/2404.05961v2#S5.T2 "Table 2 ‣ Results ‣ 5.1 LLM2Vec leads to strong performance on the MTEB leaderboard ‣ 5 Combining LLM2Vec with supervised contrastive learning ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"), we provide additional results for the recently released Meta-Llama-3-8B-Instruct model (Meta-LLaMA-3-8B, [^2]).

##### Training data

We perform both the MNTP and the unsupervised SimCSE step using data from English Wikipedia. We select data from Wikipedia as it is presumably included in the pre-training mixture of all the models we experiment with. It is therefore fair to assume that these two adaptation steps are not teaching the model any new knowledge beyond how to attend to future tokens and how to construct sequence representations. Specifically, we use the Wikitext-103 dataset [^33] for the MNTP step and a subset of Wikipedia sentences released by [^16] for the unsupervised SimCSE step.

##### Masked next token prediction

We follow established practice from the masked language modeling literature and randomly mask a fraction of the tokens from the input sequence [^13]. We use the underscore (\_) as the mask token, since the models we experiment with do not have a special token for masking. We fine-tune the model using LoRA [^19] to predict the masked token using the representation of the previous token to maximally align our training objective with the pre-training setup of decoder-only LLMs. For all models, we trained for 1000 steps with a batch size of 32 on a single 80GB A100 GPU. For 7B and 8B models, this training takes only 100 minutes. We provide additional details of our training setup and hyperparameters in [Section D.1.1](https://arxiv.org/html/2404.05961v2/2404.05961v2#A4.SS1.SSS1 "D.1.1 MNTP training details ‣ D.1 Training details ‣ Appendix D Details on unsupervised results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders").

##### Unsupervised contrastive learning

For the contrastive training, we apply the unsupervised SimCSE approach by [^16]. The positive examples are constructed by applying LLM’s dropout twice on the same input sequence, whereas the other sequences in the batch act as in-batch negatives. We merge the MNTP LoRA weights into the base model and initialize new LoRA parameters before starting the SimCSE training, which ensures that the models retains the knowledge learned in the previous step. Similar to the MNTP step, we train for 1000 steps. For 7B and 8B models, this training takes 3 hours on a single 80GB A100 GPU with a batch size of 128. We provide additional details of our training setup and hyperparameters in [Section D.1.2](https://arxiv.org/html/2404.05961v2/2404.05961v2#A4.SS1.SSS2 "D.1.2 SimCSE training details ‣ D.1 Training details ‣ Appendix D Details on unsupervised results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders").

## 3 LLM2Vec-transformed models are strong unsupervised text embedders

### 3.1 Evaluation on word-level tasks

We start by evaluating on word-level tasks to demonstrate that LLM2Vec is successful at improving the contextual representations constructed by decoder-only LLMs.

![Refer to caption](https://arxiv.org/html/2404.05961v2/x4.png)

(a) Chunking

##### Setup

We evaluate three word-level tasks: chunking, named-entity recognition (NER), and part-of-speech tagging (POS), using the CoNLL-2003 benchmark [^47]. We embed each input sentence and train a task-specific linear classifier on top of the frozen representations. This is akin to the linear probing setup commonly used in the language model analysis literature [^4]. We compare the LLM2Vec-transformed models to DeBERTa-v3-large [^17], the current state-of-the-art encoder-only model. Additional details about our setup are provided in [Section D.1.3](https://arxiv.org/html/2404.05961v2/2404.05961v2#A4.SS1.SSS3 "D.1.3 Word-level training details ‣ D.1 Training details ‣ Appendix D Details on unsupervised results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders").

##### Results

[Figure 2](https://arxiv.org/html/2404.05961v2/2404.05961v2#S3.F2 "In 3.1 Evaluation on word-level tasks ‣ 3 LLM2Vec-transformed models are strong unsupervised text embedders ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") shows the results of our evaluation (a detailed breakdown of the results is provided in [Table 4](https://arxiv.org/html/2404.05961v2/2404.05961v2#A4.T4 "In D.2.1 Word-level task results ‣ D.2 Additional results ‣ Appendix D Details on unsupervised results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders")). On each of the three tasks, constructing token representations with causal attention (Uni) already outperforms the encoder-only baseline. This is not surprising, given that the models we experiment with are significantly larger and have been pre-trained on more data. As expected, naively applying bidirectional attention dramatically hurts performance in most cases. Interestingly, for Mistral-7B, enabling bidirectional attention hurts performance much less compared to S-LLaMA-1.3B and LLaMA-2-7B. For NER, Mistral’s performance even improves by $0.6\%$ with bidirectional connections.

Focusing on the LLM2Vec-transformed models, we observe that for all models and tasks, adapting via MNTP improves performance. For instance, in the chunking task, we see improvements for S-LLaMA-1.3B (by $5\%$ ), LLaMA-2-7B (by $4\%$ ), and Mistral-7B (by $4\%$ ). Combining MNTP with SimCSE, however, performs worse than just applying MNTP. This is expected for word-level tasks, as SimCSE adapts the representations for sequence-level tasks.

### 3.2 Evaluation on sequence-level tasks

![Refer to caption](https://arxiv.org/html/2404.05961v2/x7.png)

(a) S-LLaMA-1.3B

Next, we evaluate on the Massive Text Embedding Benchmark (MTEB), a collection of 7 diverse embedding task categories covering a total of 56 datasets [^35]. To select the best-performing pooling method for each method, we perform ablations on a 15 task subset consisting of representative tasks from each of the MTEB categories. We provide additional details and justification for how we chose this subset in [Section C.1](https://arxiv.org/html/2404.05961v2/2404.05961v2#A3.SS1 "C.1 MTEB subset details ‣ Appendix C Massive Text Embeddings Benchmark (MTEB) ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders").

##### Setup

Following previous work [^45], we evaluate with task-specific instructions. For a fair comparison, we use the same set of instructions as [^51] which are also used by [^44]. The instructions are only added to queries and can be found in [Table 10](https://arxiv.org/html/2404.05961v2/2404.05961v2#A7.T10 "In G.3 Results ‣ Appendix G Details on supervised results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") of [Section C.2](https://arxiv.org/html/2404.05961v2/2404.05961v2#A3.SS2 "C.2 MTEB instructions ‣ Appendix C Massive Text Embeddings Benchmark (MTEB) ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"). For symmetric tasks, the same instruction will be used for the query and the document. When applying (weighted) mean pooling [^34], we exclude the instruction tokens.

As a baseline, we compare to the unsupervised BERT models obtained from [^16]. Additionally, we compare to Echo embeddings, a concurrent approach by [^44], which we run with the same models and instructions (see [Section E.1](https://arxiv.org/html/2404.05961v2/2404.05961v2#A5.SS1 "E.1 Reproducibility ‣ Appendix E Comparison with Echo embedding ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") for more details on our implementation of Echo embeddings). Echo duplicates the input and takes the pooling over the second occurrence to address the limitation of causal information flow.

##### Results on our 15 task subset of MTEB

[Figure 3](https://arxiv.org/html/2404.05961v2/2404.05961v2#S3.F3 "In 3.2 Evaluation on sequence-level tasks ‣ 3 LLM2Vec-transformed models are strong unsupervised text embedders ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") shows the impact of various pooling methods for all three models on the subset of MTEB tasks. We can clearly observe that applying causal attention is sub-optimal when constructing text embeddings. The dominant paradigm of applying the EOS pooling for models with causal attention is outperformed by (weighted) mean pooling. Enabling bidirectional attention without any training harms performance for S-LLaMA-1.3B and LLaMA-2-7B. Similar to our word-level results, the performance of Mistral-7B improves with bidirectional attention, even without any training.

For LLM2Vec-transformed models, applying MNTP training improves the performance of S-LLaMA-1.3B and Mistral-7B. Moreover, applying SimCSE further boosts the performance of S-LLaMA-1.3B, LLaMA-2-7B, and Mistral-7B by $49.8\%$ , $23.2\%$ , and $37.5\%$ compared to the best causal baseline on the MTEB subset. We further conduct an ablation of each component of LLM2Vec recipe in [Section D.2.2](https://arxiv.org/html/2404.05961v2/2404.05961v2#A4.SS2.SSS2 "D.2.2 Sentence-level task results ‣ D.2 Additional results ‣ Appendix D Details on unsupervised results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") ([Table 5](https://arxiv.org/html/2404.05961v2/2404.05961v2#A4.T5 "In D.2.2 Sentence-level task results ‣ D.2 Additional results ‣ Appendix D Details on unsupervised results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders")).

<table><tbody><tr><th><span>Categories</span> <math><semantics><mo>→</mo> <annotation-xml><ci>→</ci></annotation-xml> <annotation>\rightarrow</annotation> <annotation>→</annotation></semantics></math></th><td><span>Retr.</span></td><td><span>Rerank.</span></td><td><span>Clust.</span></td><td><span>PairClass.</span></td><td><span>Class.</span></td><td><span>STS</span></td><td><span>Summ.</span></td><td><span>Avg</span></td></tr><tr><th><span># of datasets</span> <math><semantics><mo>→</mo> <annotation-xml><ci>→</ci></annotation-xml> <annotation>\rightarrow</annotation> <annotation>→</annotation></semantics></math></th><td>15</td><td>4</td><td>11</td><td>3</td><td>12</td><td>10</td><td>1</td><td>56</td></tr><tr><th colspan="9"><span>Encoder-only</span></th></tr><tr><th>BERT</th><td>10.59</td><td>43.44</td><td>30.12</td><td>56.33</td><td>61.66</td><td>54.36</td><td>29.82</td><td>38.33</td></tr><tr><th>BERT + SimCSE</th><td>20.29</td><td>46.47</td><td>29.04</td><td>70.33</td><td>62.50</td><td>74.33</td><td>31.15</td><td>45.45</td></tr><tr><th colspan="9"><span>S-LLaMA-1.3B</span></th></tr><tr><th>Uni + w. Mean</th><td>9.47</td><td>38.02</td><td>28.02</td><td>42.19</td><td>59.79</td><td>49.15</td><td>24.98</td><td>35.05</td></tr><tr><th><span>LLM2Vec (w/o SimCSE)</span></th><td><span>15.48</span></td><td><span>40.99</span></td><td><span>31.83</span></td><td><span>50.63</span></td><td><span>64.54</span></td><td><span>62.06</span></td><td><span>26.82</span></td><td><span>41.43</span></td></tr><tr><th><span>LLM2Vec</span></th><td><span>25.93</span></td><td><span>47.70</span></td><td><span>37.45</span></td><td><span>72.21</span></td><td><span>67.67</span></td><td><span>71.61</span></td><td><span>31.23</span></td><td><span>49.42</span></td></tr><tr><th>Echo</th><td>10.36</td><td>41.52</td><td>30.03</td><td>52.08</td><td>63.75</td><td>59.36</td><td>22.79</td><td>39.10</td></tr><tr><th colspan="9"><span>LLaMA-2-7B</span></th></tr><tr><th>Ui + w. Mean</th><td>15.16</td><td>46.94</td><td>36.85</td><td>61.41</td><td>69.05</td><td>63.42</td><td>26.64</td><td>44.54</td></tr><tr><th><span>LLM2Vec (w/o SimCSE)</span></th><td><span>19.86</span></td><td><span>44.74</span></td><td><span>35.31</span></td><td><span>61.60</span></td><td><span>67.94</span></td><td><span>66.74</span></td><td><span>26.83</span></td><td><span>45.70</span></td></tr><tr><th><span>LLM2Vec</span></th><td><span>36.75</span></td><td><span>52.95</span></td><td><span>40.83</span></td><td><span>77.89</span></td><td><span>71.57</span></td><td><span>76.41</span></td><td><span>31.38</span></td><td><span>55.36</span></td></tr><tr><th>Echo</th><td>16.16</td><td>46.84</td><td>34.25</td><td>63.54</td><td>69.82</td><td>67.95</td><td>25.57</td><td>45.36</td></tr><tr><th colspan="9"><span>Mistral-7B</span></th></tr><tr><th>Uni + w. Mean</th><td>10.43</td><td>45.11</td><td>35.82</td><td>60.28</td><td>71.14</td><td>58.59</td><td>26.57</td><td>42.46</td></tr><tr><th>Bi + Mean</th><td>15.84</td><td>47.40</td><td>35.55</td><td>66.53</td><td>72.18</td><td>71.04</td><td>29.93</td><td>46.86</td></tr><tr><th><span>LLM2Vec (w/o SimCSE)</span></th><td><span>19.74</span></td><td><span>50.43</span></td><td><span>40.06</span></td><td><span>70.95</span></td><td><span>72.51</span></td><td><span>71.90</span></td><td><span>27.84</span></td><td><span>49.43</span></td></tr><tr><th><span>LLM2Vec</span></th><td><span>38.05</span></td><td><span>53.99</span></td><td><span>40.63</span></td><td><span>80.94</span></td><td><span>74.07</span></td><td><span>78.50</span></td><td><span>30.19</span></td><td><span>56.80</span></td></tr><tr><th>Echo</th><td>22.68</td><td>51.07</td><td>36.78</td><td>75.87</td><td>72.69</td><td>73.60</td><td>29.54</td><td>50.26</td></tr><tr><th colspan="9"><span>Meta-LLaMA-3-8B</span></th></tr><tr><th>Uni + w. Mean</th><td>15.17</td><td>46.22</td><td>36.84</td><td>60.94</td><td>67.41</td><td>62.80</td><td>25.51</td><td>43.98</td></tr><tr><th>Bi + Mean</th><td>3.90</td><td>34.56</td><td>14.27</td><td>42.71</td><td>57.89</td><td>51.15</td><td>23.26</td><td>30.56</td></tr><tr><th><span>LLM2Vec (w/o SimCSE)</span></th><td><span>24.75</span></td><td><span>49.20</span></td><td><span>39.74</span></td><td><span>65.91</span></td><td><span>69.00</span></td><td><span>67.85</span></td><td><span>25.59</span></td><td><span>48.84</span></td></tr><tr><th><span>LLM2Vec</span></th><td><span>39.19</span></td><td><span>53.09</span></td><td><span>41.99</span></td><td><span>78.01</span></td><td><span>71.88</span></td><td><span>75.86</span></td><td><span>31.45</span></td><td><span>56.23</span></td></tr><tr><th>Echo</th><td>12.58</td><td>49.79</td><td>36.32</td><td>68.95</td><td>70.22</td><td>67.43</td><td>26.44</td><td>45.32</td></tr></tbody></table>

Table 1: Unsupervised results on MTEB. We compare S-LLaMA-1.3B, LLaMA-2-7B, Mistral-7B, and Meta-LLaMA-3-8B with and without LLM2Vec to the unsupervised BERT models of [^16] as well as Echo embeddings [^44].

##### Results on full MTEB

[Table 1](https://arxiv.org/html/2404.05961v2/2404.05961v2#S3.T1 "In Results on our 15 task subset of MTEB ‣ 3.2 Evaluation on sequence-level tasks ‣ 3 LLM2Vec-transformed models are strong unsupervised text embedders ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") shows the results of the best performing models, which we select based on the ablation above, on the full MTEB dataset. After the first two steps of LLM2Vec—bidirectional attention and MNTP—we observe a considerable improvement in performance for all four models (e.g., $16.4\%$ improvement for Mistral-7B).

When comparing to Echo embeddings, LLM2Vec (the first two steps only) <sup>3</sup> <sup>3</sup> 3 We only directly compare the performance after the first two steps of LLM2Vec to Echo embeddings as applying SimCSE involves learning sequence representation, which makes the comparison unfair. leads to improved performance for S-LLaMA-1.3B, LLaMA-2-7B, and Meta-LLaMA-3-8B, and performs almost on par for Mistral-7B. However, compared to Echo embeddings, LLM2Vec is much more efficient as Echo embeddings repeat the input and therefore double the sequence length which makes inference considerably slower (we provide a runtime comparison in [Section E.2](https://arxiv.org/html/2404.05961v2/2404.05961v2#A5.SS2 "E.2 Efficiency ‣ Appendix E Comparison with Echo embedding ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders")). Adding the final step of the LLM2Vec recipe—unsupervised SimCSE—further boosts all three models by a large margin, making our LLM2Vec Mistral-7B SOTA among all unsupervised models with a score of $56.80$ .

Interestingly, Meta-LLaMA-3-8B with LLM2Vec (w/o SimCSE) outperforms echo embeddings by a larger margin compared to the other models. Adding SimCSE again boosts performance, but does not outperform LLM2Vec applied to Mistral-7B.

Overall, our results highlight that LLM2Vec is successful at transforming decoder-only LLMs into strong text embedding models which outperform previous unsupervised approaches on the challenging MTEB leaderboard.

## 4 How does LLM2Vec affect a model?

### 4.1 LLM2Vec helps models to capture information from future tokens

To analyze the extent to which LLM2Vec-transformed models incorporate information from future tokens, we adopt the analysis of [^44] and test how well the model performs at judging the similarity between sentences that share the same prefix.

##### Setup

We evaluate on a synthetic dataset collected by [^44], which consists of 35 sentence triples $\{(q_{i}$ , $s^{+}_{i}$ , $s^{-}_{i})\}_{i=1}^{35}$ with $q_{i}=(A_{i},B_{i})$ , $s^{+}_{i}=(A_{i},C_{i})$ , and $s^{-}_{i}=(A_{i},D_{i})$ , where $B_{i}$ and $C_{i}$ have a similar meaning but $B_{i}$ and $D_{i}$ don’t. We compute a sequence representation for each of these sentences by pooling only over the first part of the sentence, i.e., $A_{i}$ . We then compute the cosine similarity between the resulting embeddings. A model that incorporates information from future tokens ( $B_{i}$ , $C_{i}$ , or $D_{i}$ ) in the representations of the prefix $A_{i}$ should assign a higher similarity to the positive example.

##### Results

![Refer to caption](https://arxiv.org/html/2404.05961v2/x10.png)

(a) S-LLaMA-1.3B

![Refer to caption](https://arxiv.org/html/2404.05961v2/x14.png)

(a) S-LLaMA-1.3B

[Figure 5](https://arxiv.org/html/2404.05961v2/2404.05961v2#S4.F5 "In Results ‣ 4.1 LLM2Vec helps models to capture information from future tokens ‣ 4 How does LLM2Vec affect a model? ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") shows the results of our analysis for S-LLaMA-1.3B and Mistral-7B. Results for LLaMA-2-7B, which show the same trends, and a comparison to Echo embeddings are provided in [Appendix F](https://arxiv.org/html/2404.05961v2/2404.05961v2#A6 "Appendix F More analysis results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"). For S-LLaMA-1.3B, we observe that enabling bidirectional attention and training with the MNTP objective are sufficient to establish a clear separation between the positive and negative examples. For Mistral-7B, all setups lead to a larger cosine similarity between the query and positive than the query and negative examples.

### 4.2 Why does bidirectional attention without training work for Mistral models?

Our empirical results so far as well as the analysis above share an intriguing observation: enabling bidirectional attention works well for Mistral-7B, even without any training. Below, we investigate this surprising behavior by analyzing how bidirectional attention impacts the representations of a model.

##### Setup

We feed a single input sequence (a random paragraph from Wikipedia) to each model and compute the hidden representations of every token at every layer $l$ with causal ( $\mathbf{H}^{c}_{l}$ ) and bidirectional attention ( $\mathbf{H}^{bi}_{l}$ ). For every layer, we compute the cosine similarity between the representations constructed using causal and bidirectional attention, i.e., $\text{sim}(\mathbf{H}^{c}_{l},\mathbf{H}^{bi}_{l})$ . For most layers, we expect this similarity to be low, as enabling bidirectional attention without any training should lead to substantially different representations.

##### Results

[Figure 6](https://arxiv.org/html/2404.05961v2/2404.05961v2#S4.F6 "In Results ‣ 4.1 LLM2Vec helps models to capture information from future tokens ‣ 4 How does LLM2Vec affect a model? ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") shows that as expected, for S-LLaMA-1.3B and LLaMA-2-7B, enabling bidirectional attention without training has a profound impact on the representations, leading to low cosine similarity across almost all layers and token positions. For Mistral-7B, on the other hand, the representations have very high cosine similarity throughout.

Based on these findings (we replicate these results for other inputs and other variants of Mistral in [Appendix F](https://arxiv.org/html/2404.05961v2/2404.05961v2#A6 "Appendix F More analysis results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders")) and the strong unsupervised results for Mistral-7B with bidirectional attention, we speculate that Mistral models are pre-trained with some form bidirectional attention, e.g., prefix language modeling [^41] – at least for some parts of its training. We leave a more detailed investigation of this intriguing behavior for future work.

## 5 Combining LLM2Vec with supervised contrastive learning

The final piece of our evaluation combines LLM2Vec with supervised contrastive learning.

### 5.1 LLM2Vec leads to strong performance on the MTEB leaderboard

##### Setup

For supervised training, we train on a replication of the public portion of the E5 dataset [^51] curated by [^44]. The dataset consists of approximately 1.5M samples and we provide details on its compilation in [Section G.1](https://arxiv.org/html/2404.05961v2/2404.05961v2#A7.SS1 "G.1 E5 dataset ‣ Appendix G Details on supervised results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"). We follow standard practice and train the models with contrastive learning using hard negatives and in-batch negatives. We use LoRA fine-tuning for supervised setting as well. The MNTP LoRA weights are merged into the base model, and the trainable LoRA weights are initialized with SimCSE weights. For LLM2Vec models that use just MNTP, the LoRA weights are randomly initialized. The training is performed for 1000 steps with a batch size of 512. We detail other hyperparameters in [Section G.2](https://arxiv.org/html/2404.05961v2/2404.05961v2#A7.SS2 "G.2 Training details ‣ Appendix G Details on supervised results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders").

For a fair comparison, we only compare to models trained on publicly available data and provide a comparison to the top entries on the MTEB leaderboard in [Section G.3](https://arxiv.org/html/2404.05961v2/2404.05961v2#A7.SS3 "G.3 Results ‣ Appendix G Details on supervised results ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders").

##### Results

<table><tbody><tr><th><span>Categories</span> <math><semantics><mo>→</mo> <annotation-xml><ci>→</ci></annotation-xml> <annotation>\rightarrow</annotation> <annotation>→</annotation></semantics></math></th><td><span>Retr</span>.</td><td><span>Rerank.</span></td><td><span>Clust.</span></td><td><span>PairClass.</span></td><td><span>Class.</span></td><td><span>STS</span></td><td><span>Summ.</span></td><td><span>Avg</span></td></tr><tr><th><span># of datasets</span> <math><semantics><mo>→</mo> <annotation-xml><ci>→</ci></annotation-xml> <annotation>\rightarrow</annotation> <annotation>→</annotation></semantics></math></th><td>15</td><td>4</td><td>11</td><td>3</td><td>12</td><td>10</td><td>1</td><td>56</td></tr><tr><th colspan="9"><span>Previous work w/ public data only</span></th></tr><tr><th>Instructor-xl</th><td>49.26</td><td>57.29</td><td>44.74</td><td>86.62</td><td>73.12</td><td>83.06</td><td><span>32.32</span></td><td>61.79</td></tr><tr><th>BGE <math><semantics><msub><mtext>large-en-v1.5</mtext></msub> <annotation-xml><apply><ci><mtext>large-en-v1.5</mtext></ci></apply></annotation-xml> <annotation>{}_{\text{large-en-v1.5}}</annotation> <annotation>start_FLOATSUBSCRIPT large-en-v1.5 end_FLOATSUBSCRIPT</annotation></semantics></math></th><td>54.29</td><td>60.03</td><td>46.08</td><td>87.12</td><td>75.97</td><td>83.11</td><td>31.61</td><td>64.23</td></tr><tr><th>GritLM <math><semantics><msub><mtext>Mistral-7b-v1</mtext></msub> <annotation-xml><apply><ci><mtext>Mistral-7b-v1</mtext></ci></apply></annotation-xml> <annotation>{}_{\text{Mistral-7b-v1}}</annotation> <annotation>start_FLOATSUBSCRIPT Mistral-7b-v1 end_FLOATSUBSCRIPT</annotation></semantics></math> + public data</th><td>53.10</td><td><span>61.30</span></td><td><span>48.90</span></td><td>86.90</td><td>77.00</td><td>82.80</td><td>29.40</td><td>64.70</td></tr><tr><th>E5 <math><semantics><msub><mtext>Mistral-7b-v1</mtext></msub> <annotation-xml><apply><ci><mtext>Mistral-7b-v1</mtext></ci></apply></annotation-xml> <annotation>{}_{\text{Mistral-7b-v1}}</annotation> <annotation>start_FLOATSUBSCRIPT Mistral-7b-v1 end_FLOATSUBSCRIPT</annotation></semantics></math> + public data</th><td>52.78</td><td>60.38</td><td>47.78</td><td><span>88.47</span></td><td>76.80</td><td>83.77</td><td>31.90</td><td>64.56</td></tr><tr><th>Echo <math><semantics><msub><mtext>Mistral-7b-v1</mtext></msub> <annotation-xml><apply><ci><mtext>Mistral-7b-v1</mtext></ci></apply></annotation-xml> <annotation>{}_{\text{Mistral-7b-v1}}</annotation> <annotation>start_FLOATSUBSCRIPT Mistral-7b-v1 end_FLOATSUBSCRIPT</annotation></semantics></math></th><td>55.52</td><td>58.14</td><td>46.32</td><td>87.34</td><td><span>77.43</span></td><td>82.56</td><td>30.73</td><td>64.68</td></tr><tr><th colspan="9"><span>S-LLaMA-1.3B</span></th></tr><tr><th>Uni + w. Mean</th><td>51.02</td><td>54.65</td><td>39.90</td><td>83.57</td><td>71.64</td><td>82.16</td><td>30.05</td><td>60.44</td></tr><tr><th><span>LLM2Vec (w/o SimCSE)</span></th><td><span>51.44</span></td><td><span>55.38</span></td><td><span>43.57</span></td><td><span>86.20</span></td><td><span>72.21</span></td><td><span>83.58</span></td><td><span>30.01</span></td><td><span>61.85</span></td></tr><tr><th><span>LLM2Vec</span></th><td><span>51.49</span></td><td><span>55.58</span></td><td><span>43.24</span></td><td><span>85.80</span></td><td><span>72.98</span></td><td><span>83.62</span></td><td><span>30.12</span></td><td><span>61.96</span></td></tr><tr><th colspan="9"><span>LLaMA-2-7B</span></th></tr><tr><th>Uni + w. Mean</th><td>54.33</td><td>58.01</td><td>40.57</td><td>87.01</td><td>75.60</td><td>83.47</td><td>29.68</td><td>62.96</td></tr><tr><th><span>LLM2Vec (w/o SimCSE)</span></th><td><span>54.60</span></td><td><span>57.38</span></td><td><span>45.24</span></td><td><span>88.03</span></td><td><span>76.33</span></td><td><span>83.73</span></td><td><span>28.49</span></td><td><span>64.14</span></td></tr><tr><th><span>LLM2Vec</span></th><td><span>54.34</span></td><td><span>57.70</span></td><td><span>45.04</span></td><td><span>87.87</span></td><td><span>76.53</span></td><td><span>83.43</span></td><td><span>28.82</span></td><td><span>64.04</span></td></tr><tr><th colspan="9"><span>Mistral-7B</span></th></tr><tr><th>Uni + w. Mean</th><td>54.81</td><td>57.37</td><td>41.07</td><td>86.05</td><td>76.01</td><td>83.44</td><td>30.74</td><td>63.20</td></tr><tr><th><span>LLM2Vec (w/o SimCSE)</span></th><td><span>55.99</span></td><td><span>58.42</span></td><td><span>45.54</span></td><td><span>87.99</span></td><td><span>76.63</span></td><td><span>84.09</span></td><td><span>29.96</span></td><td><span>64.80</span></td></tr><tr><th><span>LLM2Vec</span></th><td><span>56.05</span></td><td><span>58.59</span></td><td><span>45.12</span></td><td><span>88.18</span></td><td><span>76.72</span></td><td><span>83.69</span></td><td><span>30.66</span></td><td><span>64.72</span></td></tr><tr><th colspan="9"><span>Meta-LLaMA-3-8B</span></th></tr><tr><th>Uni + w. Mean</th><td>55.42</td><td>58.60</td><td>43.19</td><td>86.29</td><td>75.56</td><td>83.95</td><td>30.59</td><td>63.87</td></tr><tr><th><span>LLM2Vec (w/o SimCSE)</span></th><td><span>56.63</span></td><td><span>59.68</span></td><td><span>46.45</span></td><td><span>87.80</span></td><td><span>75.92</span></td><td><span>83.58</span></td><td><span>30.94</span></td><td><span>65.01</span></td></tr><tr><th><span>LLM2Vec</span></th><td><span>56.71</span></td><td><span>59.02</span></td><td><span>45.86</span></td><td><span>87.95</span></td><td><span>76.67</span></td><td><span>82.98</span></td><td><span>29.67</span></td><td><span>64.90</span></td></tr></tbody></table>

Table 2: Supervised results on full MTEB benchmark. The best performing LLM2Vec model Meta-LLaMA-3-8B + LLM2Vec (w/o SimCSE) achieves a new SOTA performance among models trained only on publicly available data.

![Refer to caption](https://arxiv.org/html/2404.05961v2/x17.png)

(a) S-LLaMA-1.3B

[Table 2](https://arxiv.org/html/2404.05961v2/2404.05961v2#S5.T2 "In Results ‣ 5.1 LLM2Vec leads to strong performance on the MTEB leaderboard ‣ 5 Combining LLM2Vec with supervised contrastive learning ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders") shows the results of our evaluation. For all models, transforming a model with LLM2Vec leads to improved performance over the strong Uni + weighted mean baseline. As expected, performing unsupervised SimCSE is less crucial for supervised training, and even leads to slightly worse performance for LLaMA-2-7B, Mistral-7B, and Meta-LLaMA-3-8B compared to just performing the MNTP step of LLM2Vec (LLM2Vec w/o SimCSE). However, as we will show in [Section 5.2](https://arxiv.org/html/2404.05961v2/2404.05961v2#S5.SS2 "5.2 LLM2Vec leads to more sample-efficient training ‣ 5 Combining LLM2Vec with supervised contrastive learning ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"), LLM2Vec with MNTP and SimCSE is much more sample-efficient, and therefore crucial in compute or data-constrained settings. Notably, our best model, Meta-LLaMA-3-8B + LLM2Vec (w/o SimCSE) leads to a new state-of-the-art performance among models trained only on publicly available data.

### 5.2 LLM2Vec leads to more sample-efficient training

##### Setup

To demonstrate the sample-efficiency of LLM2Vec-transformed models, we save a checkpoint every 25 training steps and evaluate them on our 15 task subset of MTEB.

##### Results

As shown in [Figure 7](https://arxiv.org/html/2404.05961v2/2404.05961v2#S5.F7 "In Results ‣ 5.1 LLM2Vec leads to strong performance on the MTEB leaderboard ‣ 5 Combining LLM2Vec with supervised contrastive learning ‣ LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"), LLM2Vec-transformed models reach better performance earlier in training. This observation is consistent across all three models. For S-LLaMA-1.3B, the smallest of our three models, even performing just MNTP leads to a considerably improved sample-efficiency. These results are particularly encouraging for settings where it is hard to acquire high quality labeled data, a setting which we leave for future work.

## 6 Related Work

##### Supervised text encoders

Initially, supervised methods primarily relied on tasks such as natural language inference or sentence similarity to train BERT-like models for producing sentence embeddings [^10]. Subsequently, BERT-like models have also been adapted to tasks like retrieval [^23]. More recent methods have further improved these representations through a complex multi-stage learning pipeline that consists of large-scale weakly supervised contrastive training followed by multi-task fine-tuning [^38] Recent approaches have focused on enhancing the generalization and transferability of text embeddings using instructions [^45].

##### Unsupervised text encoders

Another line of work has explored training text embedders in an unsupervised manner using only a set of unordered sentences. These unsupervised approaches typically create two different representations of the same sentence for contrastive learning. The methods vary in how they form these representations – perturbing the input sentence [^53], or using different model instances [^6]. SimCSE [^16], the approach used in this work, generates two representations of the same sentence by passing it through the model twice with different dropout masks.

##### Turning decoder-only LLMs into text encoders

While decoder-only LLMs have outperformed bidirectional encoders across a large variety of language understanding tasks [^5], their impact on sentence representation learning remains limited. The most common approaches in literature use the final hidden state of the last token as the sentence embedding [^37].

There are few works that explore the limitations of using a causal attention mask when adapting decoder-only LLMs for text classification and sentence representation tasks. [^28] experiment with removing the causal mask of Llama-2 during supervised fine-tuning for text classification and NER tasks. Similarly, [^14] enable bidirectional attention for a group of layers during supervised fine-tuning on NER and chunking. In the context of sentence representation learning, [^26] explore enabling bidirectional attention in the last layer of a decoder-only model during supervised contrastive fine-tuning on STS tasks.

Concurrent to our work, several works have focused on converting decoder-only-LLMs to text encoders in supervised and unsupervised manner. [^21] and [^25] prompt the language model to summarize the input text in one word, and take the last layer’s hidden embedding for the last token as the text’s representation. [^36] perform multi-task full fine-tuning using a combination of self-supervised language modeling with causal attention and supervised contrastive learning with bidirectional attention. In contrast, our proposed approach is much more computationally efficient, as it requires only parameter-efficient fine-tuning and 1000 gradient steps. Closest to our work is the concurrent work of [^44]. They propose to copy the input sequence and append it to itself, which addresses the contextualization issue of causal attention as tokens in the copy of the input can now attend to ”future” tokens in the previous sequence. While this performs well in practice, it significantly increases the computational cost at inference time, which can be particularly problematic for encoding longer documents. Our approach outperforms [^44], without inducing any additional computational overhead at inference time.

## 7 Conclusion

We present LLM2Vec, a strong unsupervised approach to transform any decoder-only LLMs into a (universal) text embedder. We perform an extensive evaluation on word- and sequence-level tasks and demonstrate the effectiveness of LLM2Vec in both unsupervised and supervised settings. Applying LLM2Vec to Mistral-7B achieves a new state-of-the-art performance on MTEB among unsupervised approaches. When combining LLM2Vec with supervised contrastive fine-tuning, Meta-LLaMA-3-8B achieves SOTA performance among approaches that train only on publicly available data (as of May 24, 2024). Beyond our strong empirical contributions, we provide an extensive analysis of how LLM2Vec impacts the underlying model and reveal an intriguing property of Mistral-7B, which explains its strong out of the box performance with bidirectional attention. The simplicity of our approach, as well as its compute and sample-efficiency, makes LLM2vec a promising solution for low-resource and compute constrained scenarios and opens up several interesting avenues for future work.

## Acknowledgements

We thank the members of SR’s research group for providing feedback throughout the project. Furthermore, we thank Jacob Mitchell Springer for providing the supervised training data used in [^44]. PB is supported by the Mila-Intel Grant program. MM is partly funded by the Mila P2v5 Technology Maturation Grant and the Mila-Samsung grant. SR is supported by a Facebook CIFAR AI Chair and NSERC Discovery Grant program.

## References

[^1]: Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe.SemEval-2014 task 10: Multilingual semantic textual similarity.In Preslav Nakov and Torsten Zesch (eds.), *Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)*, pp. 81–91, Dublin, Ireland, August 2014. Association for Computational Linguistics.doi: 10.3115/v1/S14-2010.URL [https://aclanthology.org/S14-2010](https://aclanthology.org/S14-2010).

[^2]: AI@Meta.Llama 3 model card.2024.URL [https://github.com/meta-llama/llama3/blob/main/MODEL\_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).

[^3]: Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih.Task-aware retrieval with instructions.In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), *Findings of the Association for Computational Linguistics: ACL 2023*, pp. 3650–3675, Toronto, Canada, July 2023. Association for Computational Linguistics.doi: 10.18653/v1/2023.findings-acl.225.URL [https://aclanthology.org/2023.findings-acl.225](https://aclanthology.org/2023.findings-acl.225).

[^4]: Yonatan Belinkov.Probing classifiers: Promises, shortcomings, and advances.*Computational Linguistics*, 48(1):207–219, March 2022.doi: 10.1162/coli˙a˙00422.URL [https://aclanthology.org/2022.cl-1.7](https://aclanthology.org/2022.cl-1.7).

[^5]: Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.Language models are few-shot learners.In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020.URL [https://proceedings.neurips.cc/paper\_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).

[^6]: Fredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylipää Hellqvist, and Magnus Sahlgren.Semantic re-tuning with contrastive tension.In *International Conference on Learning Representations*, 2021.URL [https://openreview.net/forum?id=Ov\_sMNau-PF](https://openreview.net/forum?id=Ov_sMNau-PF).

[^7]: Jianpeng Cheng, Li Dong, and Mirella Lapata.Long short-term memory-networks for machine reading.In Jian Su, Kevin Duh, and Xavier Carreras (eds.), *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, pp. 551–561, Austin, Texas, November 2016. Association for Computational Linguistics.doi: 10.18653/v1/D16-1053.URL [https://aclanthology.org/D16-1053](https://aclanthology.org/D16-1053).

[^8]: Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.PaLM: Scaling language modeling with pathways.*Journal of Machine Learning Research*, 24(240):1–113, 2023.URL [http://jmlr.org/papers/v24/22-1144.html](http://jmlr.org/papers/v24/22-1144.html).

[^9]: Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.Electra: Pre-training text encoders as discriminators rather than generators.In *International Conference on Learning Representations*, 2020.URL [https://openreview.net/forum?id=r1xMH1BtvB](https://openreview.net/forum?id=r1xMH1BtvB).

[^10]: Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes.Supervised learning of universal sentence representations from natural language inference data.In Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing*, pp. 670–680, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.doi: 10.18653/v1/D17-1070.URL [https://aclanthology.org/D17-1070](https://aclanthology.org/D17-1070).

[^11]: Tri Dao.FlashAttention-2: Faster attention with better parallelism and work partitioning.In *The Twelfth International Conference on Learning Representations*, 2024.URL [https://openreview.net/forum?id=mZn2Xyh9Ec](https://openreview.net/forum?id=mZn2Xyh9Ec).

[^12]: hilfialkaff DataCanary, Jiang Lili, Risdal Meg, Dandekar Nikhil, and tomtung.Quora question pairs.2017.URL [https://kaggle.com/competitions/quora-question-pairs](https://kaggle.com/competitions/quora-question-pairs).

[^13]: Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.BERT: Pre-training of deep bidirectional transformers for language understanding.In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.doi: 10.18653/v1/N19-1423.URL [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).

[^14]: David Dukić and Jan Šnajder.Looking right is sometimes right: Investigating the capabilities of decoder-only llms for sequence labeling.*arXiv preprint*, 2024.URL [https://arxiv.org/abs/2401.14556](https://arxiv.org/abs/2401.14556).

[^15]: Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.ELI5: Long form question answering.In Anna Korhonen, David Traum, and Lluís Màrquez (eds.), *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pp. 3558–3567, Florence, Italy, July 2019. Association for Computational Linguistics.doi: 10.18653/v1/P19-1346.URL [https://aclanthology.org/P19-1346](https://aclanthology.org/P19-1346).

[^16]: Tianyu Gao, Xingcheng Yao, and Danqi Chen.SimCSE: Simple contrastive learning of sentence embeddings.In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pp. 6894–6910, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.doi: 10.18653/v1/2021.emnlp-main.552.URL [https://aclanthology.org/2021.emnlp-main.552](https://aclanthology.org/2021.emnlp-main.552).

[^17]: Pengcheng He, Jianfeng Gao, and Weizhu Chen.DeBERTav3: Improving deBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing.In *The Eleventh International Conference on Learning Representations*, 2023.URL [https://openreview.net/forum?id=sE7-XhLxHA](https://openreview.net/forum?id=sE7-XhLxHA).

[^18]: Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang.DuReader: a Chinese machine reading comprehension dataset from real-world applications.In Eunsol Choi, Minjoon Seo, Danqi Chen, Robin Jia, and Jonathan Berant (eds.), *Proceedings of the Workshop on Machine Reading for Question Answering*, pp. 37–46, Melbourne, Australia, July 2018. Association for Computational Linguistics.doi: 10.18653/v1/W18-2605.URL [https://aclanthology.org/W18-2605](https://aclanthology.org/W18-2605).

[^19]: Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.LoRA: Low-rank adaptation of large language models.In *International Conference on Learning Representations*, 2022.URL [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9).

[^20]: Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.Mistral 7B.*arXiv preprint*, 2023a.URL [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825).

[^21]: Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang.Scaling sentence embeddings with large language models.2023b.URL [https://arxiv.org/abs/2307.16645](https://arxiv.org/abs/2307.16645).

[^22]: Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.In Regina Barzilay and Min-Yen Kan (eds.), *Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.doi: 10.18653/v1/P17-1147.URL [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).

[^23]: Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.Dense passage retrieval for open-domain question answering.In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pp. 6769–6781, Online, November 2020. Association for Computational Linguistics.doi: 10.18653/v1/2020.emnlp-main.550.URL [https://aclanthology.org/2020.emnlp-main.550](https://aclanthology.org/2020.emnlp-main.550).

[^24]: Omar Khattab and Matei Zaharia.ColBERT: Efficient and effective passage search via contextualized late interaction over bert.In *Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval*, SIGIR ’20, pp. 39–48, New York, NY, USA, 2020. Association for Computing Machinery.ISBN 9781450380164.doi: 10.1145/3397271.3401075.URL [https://doi.org/10.1145/3397271.3401075](https://doi.org/10.1145/3397271.3401075).

[^25]: Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, and Andrew Yates.Meta-task prompting elicits embeddings from large language models.2024.URL [https://arxiv.org/abs/2402.18458](https://arxiv.org/abs/2402.18458).

[^26]: Xianming Li and Jing Li.BeLLM: Backward dependency enhanced large language model for sentence embeddings.In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), *Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)*, pp. 792–804, Mexico City, Mexico, June 2024. Association for Computational Linguistics.doi: 10.18653/v1/2024.naacl-long.45.URL [https://aclanthology.org/2024.naacl-long.45](https://aclanthology.org/2024.naacl-long.45).

[^27]: Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.Towards general text embeddings with multi-stage contrastive learning.*arXiv preprint*, 2023a.URL [https://arxiv.org/abs/2308.03281](https://arxiv.org/abs/2308.03281).

[^28]: Zongxi Li, Xianming Li, Yuzhang Liu, Haoran Xie, Jing Li, Fu lee Wang, Qing Li, and Xiaoqin Zhong.Label supervised llama finetuning.*arXiv preprint*, 2023b.URL [https://arxiv.org/abs/2310.01208](https://arxiv.org/abs/2310.01208).

[^29]: Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.A structured self-attentive sentence embedding.In *International Conference on Learning Representations*, 2017.URL [https://openreview.net/forum?id=BJC\_jUqxe](https://openreview.net/forum?id=BJC_jUqxe).

[^30]: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.RoBERTa: A robustly optimized BERT pretraining approach.*arXiv preprint*, 2019.URL [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692).

[^31]: Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, and Rui Yan.Are we falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse.*arXiv preprint*, 2023.URL [https://arxiv.org/abs/2311.07468](https://arxiv.org/abs/2311.07468).

[^32]: Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin.Fine-tuning LLaMA for multi-stage text retrieval.*arXiv preprint*, 2023.URL [https://arxiv.org/abs/2310.08319](https://arxiv.org/abs/2310.08319).

[^33]: Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.Pointer sentinel mixture models.In *International Conference on Learning Representations*, 2017.URL [https://openreview.net/forum?id=Byj72udxe](https://openreview.net/forum?id=Byj72udxe).

[^34]: Niklas Muennighoff.SGPT: GPT sentence embeddings for semantic search.*arXiv preprint*, 2022.URL [https://arxiv.org/abs/2202.08904](https://arxiv.org/abs/2202.08904).

[^35]: Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers.MTEB: Massive text embedding benchmark.In Andreas Vlachos and Isabelle Augenstein (eds.), *Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics*, pp. 2014–2037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics.doi: 10.18653/v1/2023.eacl-main.148.URL [https://aclanthology.org/2023.eacl-main.148](https://aclanthology.org/2023.eacl-main.148).

[^36]: Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela.Generative representational instruction tuning.*arXiv preprint*, 2024.URL [https://arxiv.org/abs/2402.09906](https://arxiv.org/abs/2402.09906).

[^37]: Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr, Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng.Text and code embeddings by contrastive pre-training.*arXiv preprint*, 2022.URL [https://arxiv.org/abs/2201.10005](https://arxiv.org/abs/2201.10005).

[^38]: Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang.Large dual encoders are generalizable retrievers.In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pp. 9844–9855, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.doi: 10.18653/v1/2022.emnlp-main.669.URL [https://aclanthology.org/2022.emnlp-main.669](https://aclanthology.org/2022.emnlp-main.669).

[^39]: Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.Training language models to follow instructions with human feedback.*arXiv preprint*, 2022.URL [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155).

[^40]: Romain Paulus, Caiming Xiong, and Richard Socher.A deep reinforced model for abstractive summarization.In *International Conference on Learning Representations*, 2018.URL [https://openreview.net/forum?id=HkAClQgA-](https://openreview.net/forum?id=HkAClQgA-).

[^41]: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.Exploring the limits of transfer learning with a unified text-to-text transformer.*Journal of Machine Learning Research*, 21(140):1–67, 2020.URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).

[^42]: Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.SQuAD: 100,000+ questions for machine comprehension of text.In Jian Su, Kevin Duh, and Xavier Carreras (eds.), *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, pp. 2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics.doi: 10.18653/v1/D16-1264.URL [https://aclanthology.org/D16-1264](https://aclanthology.org/D16-1264).

[^43]: Nils Reimers and Iryna Gurevych.Sentence-BERT: Sentence embeddings using Siamese BERT-networks.In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pp. 3982–3992, Hong Kong, China, November 2019. Association for Computational Linguistics.doi: 10.18653/v1/D19-1410.URL [https://aclanthology.org/D19-1410](https://aclanthology.org/D19-1410).

[^44]: Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan.Repetition improves language model embeddings.*arXiv preprint*, 2024.URL [https://arxiv.org/abs/2402.15449](https://arxiv.org/abs/2402.15449).

[^45]: Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu.One embedder, any task: Instruction-finetuned text embeddings.In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), *Findings of the Association for Computational Linguistics: ACL 2023*, pp. 1102–1121, Toronto, Canada, July 2023. Association for Computational Linguistics.doi: 10.18653/v1/2023.findings-acl.71.URL [https://aclanthology.org/2023.findings-acl.71](https://aclanthology.org/2023.findings-acl.71).

[^46]: James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.FEVER: A large-scale dataset for fact extraction and VERification.In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pp. 809–819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.doi: 10.18653/v1/N18-1074.URL [https://aclanthology.org/N18-1074](https://aclanthology.org/N18-1074).

[^47]: Erik F. Tjong Kim Sang and Fien De Meulder.Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.In *Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003*, pp. 142–147, 2003.URL [https://www.aclweb.org/anthology/W03-0419](https://www.aclweb.org/anthology/W03-0419).

[^48]: Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, D. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, A. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.Llama 2: Open foundation and fine-tuned chat models.*preprint*, 2023.URL [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).

[^49]: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.Attention is all you need.In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc., 2017.URL [https://proceedings.neurips.cc/paper\_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).

[^50]: Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei.Text embeddings by weakly-supervised contrastive pre-training.*arXiv preprint*, 2022a.URL [https://arxiv.org/abs/2212.03533](https://arxiv.org/abs/2212.03533).

[^51]: Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei.Improving text embeddings with large language models.*arXiv preprint*, 2023.URL [https://arxiv.org/abs/2401.00368](https://arxiv.org/abs/2401.00368).

[^52]: Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen.Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks.In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pp. 5085–5109, Abu Dhabi, United Arab Emirates, December 2022b. Association for Computational Linguistics.doi: 10.18653/v1/2022.emnlp-main.340.URL [https://aclanthology.org/2022.emnlp-main.340](https://aclanthology.org/2022.emnlp-main.340).

[^53]: Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma.CLEAR: Contrastive learning for sentence representation.*arXiv preprint*, 2020.URL [https://arxiv.org/abs/2012.15466](https://arxiv.org/abs/2012.15466).

[^54]: Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen.Sheared LLaMA: Accelerating language model pre-training via structured pruning.In *Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@NeurIPS 2023)*, 2023.URL [https://openreview.net/forum?id=6s77hjBNfS](https://openreview.net/forum?id=6s77hjBNfS).

[^55]: Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.C-Pack: Packaged resources to advance general chinese embedding.*arXiv preprint*, 2023.URL [https://arxiv.org/abs/2309.07597](https://arxiv.org/abs/2309.07597).

[^56]: Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv, Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li, Haitao Li, Yiqun Liu, and Jin Ma.T2ranking: A large-scale chinese benchmark for passage ranking.In *Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval*, SIGIR ’23, pp. 2681–2690, New York, NY, USA, 2023. Association for Computing Machinery.ISBN 9781450394086.doi: 10.1145/3539618.3591874.URL [https://doi.org/10.1145/3539618.3591874](https://doi.org/10.1145/3539618.3591874).

[^57]: Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning.HotpotQA: A dataset for diverse, explainable multi-hop question answering.In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.), *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.doi: 10.18653/v1/D18-1259.URL [https://aclanthology.org/D18-1259](https://aclanthology.org/D18-1259).

[^58]: Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin.Mr. TyDi: A multi-lingual benchmark for dense retrieval.In Duygu Ataman, Alexandra Birch, Alexis Conneau, Orhan Firat, Sebastian Ruder, and Gozde Gul Sahin (eds.), *Proceedings of the 1st Workshop on Multilingual Representation Learning*, pp. 127–137, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.doi: 10.18653/v1/2021.mrl-1.12.URL [https://aclanthology.org/2021.mrl-1.12](https://aclanthology.org/2021.mrl-1.12).

[^59]: Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages.*Transactions of the Association for Computational Linguistics*, 11:1114–1131, 09 2023.ISSN 2307-387X.doi: 10.1162/tacl˙a˙00595.URL [https://doi.org/10.1162/tacl\_a\_00595](https://doi.org/10.1162/tacl_a_00595).