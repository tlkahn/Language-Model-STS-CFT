{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval W&B Playground\n",
    "\n",
    "Explore MTEB result JSON structure and find the right wandb logging pattern.\n",
    "\n",
    "**Problem:** The STS eval script logs per-task metrics to wandb, but the BIOSSES pane on the dashboard shows no data.\n",
    "\n",
    "**Goal:** Figure out the correct way to parse intermediary results and log them so wandb renders charts properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and explore MTEB result JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path(\"results/minicpm/sts\")\n",
    "result_files = sorted(results_dir.glob(\"*.json\"))\n",
    "print(f\"Found {len(result_files)} result files:\")\n",
    "for f in result_files:\n",
    "    print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load first available result and show full structure\n",
    "sample_file = result_files[0]\n",
    "with open(sample_file) as f:\n",
    "    sample = json.load(f)\n",
    "\n",
    "print(f\"=== {sample_file.name} ===\")\n",
    "print(json.dumps(sample, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top-level keys and split keys for every result file\n",
    "for f in result_files:\n",
    "    with open(f) as fh:\n",
    "        data = json.load(fh)\n",
    "    top_keys = list(data.keys())\n",
    "    # split keys are anything that's not metadata\n",
    "    meta_keys = {\"mteb_version\", \"dataset_revision\", \"mteb_dataset_name\"}\n",
    "    split_keys = [k for k in top_keys if k not in meta_keys]\n",
    "    print(f\"{f.stem:25s}  splits={split_keys}  metric_keys={list(data[split_keys[0]].keys()) if split_keys else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulate what `evaluation.run()` returns\n",
    "\n",
    "The MTEB `.run()` method returns a dict keyed by task name, with the same structure as the JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct what evaluation.run() returns from saved JSONs\n",
    "all_results = {}\n",
    "for f in result_files:\n",
    "    with open(f) as fh:\n",
    "        data = json.load(fh)\n",
    "    task_name = data[\"mteb_dataset_name\"]\n",
    "    all_results[task_name] = data\n",
    "\n",
    "print(f\"Tasks loaded: {list(all_results.keys())}\")\n",
    "print()\n",
    "\n",
    "# Show how our current parsing works\n",
    "for task_name, task_data in all_results.items():\n",
    "    # This is what the eval script does:\n",
    "    task_scores = task_data  # results.get(task) returns this\n",
    "    \n",
    "    # Try splits in order\n",
    "    metrics = None\n",
    "    for split in (\"test\", \"validation\"):\n",
    "        if split in task_scores:\n",
    "            metrics = task_scores[split]\n",
    "            break\n",
    "    \n",
    "    if metrics:\n",
    "        cos_sim = metrics.get(\"cos_sim\", {})\n",
    "        spearman = cos_sim.get(\"spearman\")\n",
    "        pearson = cos_sim.get(\"pearson\")\n",
    "        print(f\"{task_name:20s}  spearman={spearman:.4f}  pearson={pearson:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. W&B logging experiments\n",
    "\n",
    "The current script does:\n",
    "```python\n",
    "wandb.log({f\"{task}/cos_sim_spearman\": spearman})\n",
    "```\n",
    "\n",
    "This creates separate wandb steps per task. Possible issues:\n",
    "- Each `wandb.log()` call increments the step counter, so metrics end up at different x-axis positions\n",
    "- wandb auto-creates panels per unique metric prefix, but single-point metrics may not render well\n",
    "\n",
    "Let's try different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach A: Current approach — per-task `wandb.log()` calls (one step per task)\n",
    "\n",
    "Each task gets its own step. Only one metric per step has a value; the rest are missing. wandb may not render this well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_a = wandb.init(project=\"LM-STS-CFT\", name=\"logging-test-A-per-step\", job_type=\"debug\", reinit=True)\n",
    "\n",
    "for task_name, task_data in all_results.items():\n",
    "    metrics = task_data.get(\"test\", task_data.get(\"validation\", {}))\n",
    "    cos_sim = metrics.get(\"cos_sim\", {})\n",
    "    spearman = cos_sim.get(\"spearman\")\n",
    "    pearson = cos_sim.get(\"pearson\")\n",
    "    if spearman is not None:\n",
    "        wandb.log({f\"{task_name}/cos_sim_spearman\": spearman, f\"{task_name}/cos_sim_pearson\": pearson})\n",
    "\n",
    "wandb.finish()\n",
    "print(f\"Run A URL: {run_a.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach B: Single `wandb.log()` with all tasks at once\n",
    "\n",
    "Log everything in one step so all metrics coexist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_b = wandb.init(project=\"LM-STS-CFT\", name=\"logging-test-B-single-step\", job_type=\"debug\", reinit=True)\n",
    "\n",
    "log_dict = {}\n",
    "for task_name, task_data in all_results.items():\n",
    "    metrics = task_data.get(\"test\", task_data.get(\"validation\", {}))\n",
    "    cos_sim = metrics.get(\"cos_sim\", {})\n",
    "    spearman = cos_sim.get(\"spearman\")\n",
    "    pearson = cos_sim.get(\"pearson\")\n",
    "    if spearman is not None:\n",
    "        log_dict[f\"{task_name}/cos_sim_spearman\"] = spearman\n",
    "        log_dict[f\"{task_name}/cos_sim_pearson\"] = pearson\n",
    "\n",
    "wandb.log(log_dict)\n",
    "wandb.finish()\n",
    "print(f\"Run B URL: {run_b.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach C: `wandb.summary` (no step history, just final values)\n",
    "\n",
    "Summary metrics appear in the run overview and are ideal for single-value metrics like eval scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_c = wandb.init(project=\"LM-STS-CFT\", name=\"logging-test-C-summary\", job_type=\"debug\", reinit=True)\n",
    "\n",
    "all_spearman = {}\n",
    "for task_name, task_data in all_results.items():\n",
    "    metrics = task_data.get(\"test\", task_data.get(\"validation\", {}))\n",
    "    cos_sim = metrics.get(\"cos_sim\", {})\n",
    "    spearman = cos_sim.get(\"spearman\")\n",
    "    pearson = cos_sim.get(\"pearson\")\n",
    "    if spearman is not None:\n",
    "        wandb.summary[f\"{task_name}/cos_sim_spearman\"] = spearman\n",
    "        wandb.summary[f\"{task_name}/cos_sim_pearson\"] = pearson\n",
    "        all_spearman[task_name] = spearman\n",
    "\n",
    "if all_spearman:\n",
    "    wandb.summary[\"avg_spearman\"] = sum(all_spearman.values()) / len(all_spearman)\n",
    "\n",
    "wandb.finish()\n",
    "print(f\"Run C URL: {run_c.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach D: `wandb.Table` + bar chart\n",
    "\n",
    "Log a table and let wandb render a bar chart from it. This is the most explicit approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_d = wandb.init(project=\"LM-STS-CFT\", name=\"logging-test-D-table\", job_type=\"debug\", reinit=True)\n",
    "\n",
    "table = wandb.Table(columns=[\"task\", \"cos_sim_spearman\", \"cos_sim_pearson\"])\n",
    "all_spearman = {}\n",
    "\n",
    "for task_name, task_data in all_results.items():\n",
    "    metrics = task_data.get(\"test\", task_data.get(\"validation\", {}))\n",
    "    cos_sim = metrics.get(\"cos_sim\", {})\n",
    "    spearman = cos_sim.get(\"spearman\")\n",
    "    pearson = cos_sim.get(\"pearson\")\n",
    "    if spearman is not None:\n",
    "        table.add_data(task_name, spearman, pearson)\n",
    "        all_spearman[task_name] = spearman\n",
    "\n",
    "wandb.log({\"sts_results\": table})\n",
    "\n",
    "# Also log a bar chart directly\n",
    "bar_chart = wandb.plot.bar(\n",
    "    table, \"task\", \"cos_sim_spearman\",\n",
    "    title=\"STS Cosine Similarity Spearman by Task\"\n",
    ")\n",
    "wandb.log({\"sts_spearman_bar\": bar_chart})\n",
    "\n",
    "if all_spearman:\n",
    "    wandb.summary[\"avg_spearman\"] = sum(all_spearman.values()) / len(all_spearman)\n",
    "\n",
    "wandb.finish()\n",
    "print(f\"Run D URL: {run_d.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach E: Combined — summary for scalar values + table for overview\n",
    "\n",
    "Use `wandb.summary` for per-task scalars (shows in run overview), and a `wandb.Table` + bar chart for the visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_e = wandb.init(project=\"LM-STS-CFT\", name=\"logging-test-E-combined\", job_type=\"debug\", reinit=True)\n",
    "\n",
    "table = wandb.Table(columns=[\"task\", \"cos_sim_spearman\", \"cos_sim_pearson\", \"eval_time\"])\n",
    "all_spearman = {}\n",
    "\n",
    "for task_name, task_data in all_results.items():\n",
    "    metrics = task_data.get(\"test\", task_data.get(\"validation\", {}))\n",
    "    cos_sim = metrics.get(\"cos_sim\", {})\n",
    "    spearman = cos_sim.get(\"spearman\")\n",
    "    pearson = cos_sim.get(\"pearson\")\n",
    "    eval_time = metrics.get(\"evaluation_time\")\n",
    "    \n",
    "    if spearman is not None:\n",
    "        # Summary: per-task scalars\n",
    "        wandb.summary[f\"{task_name}/cos_sim_spearman\"] = spearman\n",
    "        wandb.summary[f\"{task_name}/cos_sim_pearson\"] = pearson\n",
    "        if eval_time is not None:\n",
    "            wandb.summary[f\"{task_name}/eval_time\"] = eval_time\n",
    "        \n",
    "        # Table row\n",
    "        table.add_data(task_name, spearman, pearson, eval_time)\n",
    "        all_spearman[task_name] = spearman\n",
    "\n",
    "# Aggregate\n",
    "if all_spearman:\n",
    "    avg = sum(all_spearman.values()) / len(all_spearman)\n",
    "    wandb.summary[\"avg_spearman\"] = avg\n",
    "\n",
    "# Table + bar chart\n",
    "wandb.log({\"sts_results\": table})\n",
    "wandb.log({\"sts_spearman_bar\": wandb.plot.bar(\n",
    "    table, \"task\", \"cos_sim_spearman\",\n",
    "    title=\"STS Cosine Similarity Spearman by Task\"\n",
    ")})\n",
    "\n",
    "wandb.finish()\n",
    "print(f\"Run E URL: {run_e.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare results on the W&B dashboard\n",
    "\n",
    "Open each run URL above and check:\n",
    "\n",
    "| Approach | Charts tab | Overview/Summary | Notes |\n",
    "|----------|-----------|-----------------|-------|\n",
    "| A (per-step) | Each task gets its own panel with one point | Metrics in summary | Current approach — sparse charts |\n",
    "| B (single-step) | All metrics logged at step 0 | Metrics in summary | Slightly better — all at same step |\n",
    "| C (summary-only) | No charts (summary doesn't create history) | All metrics visible | Clean summary, no charts tab |\n",
    "| D (table) | Bar chart panel | avg in summary | Best visual — explicit bar chart |\n",
    "| E (combined) | Bar chart panel | Per-task + avg in summary | Best of both worlds |\n",
    "\n",
    "**Pick the winner and update the eval scripts accordingly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
